# Feedback Service Configuration
# Copy to .env and fill in your values
# Built by Blue Fermion Labs - https://bluefermionlabs.com

# =============================================================================
# Server Configuration
# =============================================================================

# HTTP port (default: 8080)
PORT=8080

# SQLite database path (default: feedback.db)
FEEDBACK_DB_PATH=feedback.db

# Enable debug logging (default: false)
COMMON_DEBUG=false

# =============================================================================
# LLM Configuration (for analysis and guards)
# =============================================================================

# LLM API key - required for LLM analysis and safety guards
LLM_API_KEY=your-api-key-here

# LLM provider base URL (default: https://api.groq.com/openai/v1)
# Other options:
#   - https://api.demeterics.com/chat/v1
#   - https://api.openai.com/v1
#   - https://api.anthropic.com/v1
LLM_BASE_URL=https://api.groq.com/openai/v1

# LLM model for feedback analysis
# For Groq: llama-3.3-70b-versatile, qwen/qwen3-32b, moonshotai/kimi-k2-instruct-0905
# For OpenAI: gpt-5, o3, o4-mini, gpt-4.1
# For Anthropic: claude-opus-4-5-20251101, claude-sonnet-4-5-20250514
LLM_MODEL=llama-3.3-70b-versatile

# =============================================================================
# Self-Healing Configuration
# =============================================================================

# Enable self-healing analysis (default: false)
# When enabled, bug reports from admin users trigger automatic LLM analysis
OPENCODE_ENABLED=false

# Self-healing mode (default: analyze)
#   analyze  - Simple LLM analysis directly in Go (no Docker required)
#              Fast, lightweight, provides bug analysis and suggestions
#   opencode - Full OpenCode integration via Docker container
#              Can read codebase, suggest specific fixes, create PRs
#              Requires: make opencode-start
SELFHEALING_MODE=analyze

# Source directory for file access (analyze mode reads files from here)
# In opencode mode, this is mounted into Docker at /workspace
#
# SECURITY: The LLM can only read files within this directory.
# Set this to your project's source code root.
#
# Examples:
#   SOURCE_DIR=./src                           # Only allow access to src/
#   SOURCE_DIR=./internal                      # Only allow access to internal/
#   SOURCE_DIR=.                               # Allow access to entire repo (less secure)
#   SOURCE_DIR=/home/user/myproject/src       # Absolute path to another project
SOURCE_DIR=.

# (OpenCode mode only) Repository root to mount in Docker
# Usually the same as SOURCE_DIR or its parent
OPENCODE_REPO_DIR=.

# Comma-separated list of admin emails who can trigger self-healing
# Only feedback submitted by these users will trigger analysis
# REQUIRED if OPENCODE_ENABLED=true
ADMIN_EMAILS=your@email.com

# Feedback types that trigger analysis (default: bug)
# Options: bug, feature, improvement, question, other
# Use "all" to enable for all types
SELFHEALING_TYPES=all

# Docker container name for OpenCode (default: opencode-selfhealing)
OPENCODE_CONTAINER=opencode-selfhealing

# GitHub token for PR creation (optional, for future auto-fix features)
# Create at: https://github.com/settings/tokens
# Required scopes: repo, workflow
GITHUB_TOKEN=

# Git user for OpenCode commits (opencode mode only)
# These are used when OpenCode creates commits/PRs for automated fixes
GIT_USER_NAME=OpenCode Bot
GIT_USER_EMAIL=opencode@example.com

# Skip LLM safety guards (default: false, set true only for testing)
SKIP_GUARDS=false

# Dry run mode - log analysis but don't execute fixes (default: false)
DRY_RUN=false
